{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastparquet -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import average_precision_score as APS\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "import yaml\n",
    "import random\n",
    "import shutil\n",
    "from time import time\n",
    "import typing as tp\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.cuda import amp\n",
    "\n",
    "\n",
    "\n",
    "import timm\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# use one device only\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CFG:\n",
    "    \n",
    "#     TRAIN_ENC_PATH = Path('../../data/external/train_enc.parquet')\n",
    "#     TEST_ENC_PATH = Path('../../data/external/test_enc.parquet')\n",
    "#     TEST_PATH = Path('../../data/raw/test.parquet')\n",
    "    \n",
    "#     seed = 42\n",
    "#     deterministic = True \n",
    "\n",
    "#     PREPROCESS = False\n",
    "#     EPOCHS = 30 #20\n",
    "#     BATCH_SIZE = 4096\n",
    "#     LR = 1e-3\n",
    "#     WD = 1e-6 \n",
    "\n",
    "#     NBR_FOLDS = 15\n",
    "#     SELECTED_FOLDS = [0]\n",
    "\n",
    "#     SEED = 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configuration (CFG) should be defined here\n",
    "class CFG:\n",
    "    LR = 0.001\n",
    "    WD = 1e-4\n",
    "    NBR_FOLDS = 5\n",
    "    SELECTED_FOLDS = [0, 1, 2, 3, 4]\n",
    "    TRAIN_ENC_PATH = Path('../../data/external/train_enc.parquet')\n",
    "    TEST_ENC_PATH = Path('../../data/external/test_enc.parquet')\n",
    "    TEST_PATH = Path('../../data/raw/test.parquet')\n",
    "    BATCH_SIZE = 128\n",
    "    EPOCHS = 50\n",
    "    PATIENCE = 5\n",
    "    REDUCE_LR_PATIENCE = 3\n",
    "    REDUCE_LR_FACTOR = 0.5\n",
    "\n",
    "# Define model\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, inp_len=142, num_filters=32, hidden_dim=128, vocab_size=36, embedding_dim=128):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, num_filters, kernel_size=3, padding=0)\n",
    "        self.conv2 = nn.Conv1d(num_filters, num_filters*2, kernel_size=3, padding=0)\n",
    "        self.conv3 = nn.Conv1d(num_filters*2, num_filters*3, kernel_size=3, padding=0)\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc1 = nn.Linear(num_filters*3, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.output = nn.Linear(512, 3)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).permute(0, 2, 1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.global_max_pool(x).squeeze(2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "def get_model():\n",
    "    model = MyModel()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CFG.LR, weight_decay=CFG.WD)\n",
    "    criterion = nn.BCELoss()\n",
    "    return model, optimizer, criterion\n",
    "\n",
    "def create_dataloader(X, y=None, batch_size=32, shuffle=False):\n",
    "    X_tensor = torch.tensor(X, dtype=torch.long)\n",
    "    if y is not None:\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    else:\n",
    "        dataset = TensorDataset(X_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting seed in each env\n",
    "def set_random_seed(seed: int = 42, deterministic: bool = False):\n",
    "    \"\"\"Set seeds\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = deterministic  # type: ignore\n",
    "\n",
    "# function to set tensor to device\n",
    "def to_device(\n",
    "    tensors: tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor]],\n",
    "    device: torch.device, *args, **kwargs\n",
    "):\n",
    "    if isinstance(tensors, tuple):\n",
    "        return (t.to(device, *args, **kwargs) for t in tensors)\n",
    "    elif isinstance(tensors, dict):\n",
    "        return {\n",
    "            k: t.to(device, *args, **kwargs) for k, t in tensors.items()}\n",
    "    else:\n",
    "        return tensors.to(device, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 89440106368 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m X_val \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mloc[valid_idx, FEATURES]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     19\u001b[0m y_val \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mloc[valid_idx, TARGETS]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m---> 21\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m create_dataloader(X_val, y_val, batch_size\u001b[38;5;241m=\u001b[39mCFG\u001b[38;5;241m.\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m model, optimizer, criterion \u001b[38;5;241m=\u001b[39m get_model()\n",
      "Cell \u001b[0;32mIn[5], line 62\u001b[0m, in \u001b[0;36mcreate_dataloader\u001b[0;34m(X, y, batch_size, shuffle)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_dataloader\u001b[39m(X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 62\u001b[0m     X_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m         y_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 89440106368 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "FEATURES = [f'enc{i}' for i in range(142)]\n",
    "TARGETS = ['bind1', 'bind2', 'bind3']\n",
    "skf = StratifiedKFold(n_splits=CFG.NBR_FOLDS, shuffle=True, random_state=42)\n",
    "train = pd.read_parquet(CFG.TRAIN_ENC_PATH)\n",
    "test = pd.read_parquet(CFG.TEST_ENC_PATH)\n",
    "\n",
    "all_preds = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train[TARGETS].sum(1))):\n",
    "    \n",
    "    if fold not in CFG.SELECTED_FOLDS:\n",
    "        continue\n",
    "    \n",
    "    X_train = train.loc[train_idx, FEATURES].values\n",
    "    y_train = train.loc[train_idx, TARGETS].values\n",
    "    X_val = train.loc[valid_idx, FEATURES].values\n",
    "    y_val = train.loc[valid_idx, TARGETS].values\n",
    "    \n",
    "    train_loader = create_dataloader(X_train, y_train, batch_size=CFG.BATCH_SIZE, shuffle=True)\n",
    "    val_loader = create_dataloader(X_val, y_val, batch_size=CFG.BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model, optimizer, criterion = get_model()\n",
    "    model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(CFG.EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{CFG.EPOCHS}, Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'model-{fold}.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= CFG.PATIENCE:\n",
    "                print('Early stopping triggered')\n",
    "                break\n",
    "        \n",
    "        if patience_counter >= CFG.REDUCE_LR_PATIENCE:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= CFG.REDUCE_LR_FACTOR\n",
    "            patience_counter = 0\n",
    "    \n",
    "    model.load_state_dict(torch.load(f'model-{fold}.pt'))\n",
    "    model.eval()\n",
    "    \n",
    "    val_preds = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            preds = model(X_batch)\n",
    "            val_preds.append(preds.cpu().numpy())\n",
    "    \n",
    "    val_preds = np.concatenate(val_preds)\n",
    "    print('fold:', fold, 'CV score =', APS(y_val, val_preds, average='micro'))\n",
    "    \n",
    "    test_loader = create_dataloader(test[FEATURES].values, batch_size=2*CFG.BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    test_preds = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            preds = model(X_batch)\n",
    "            test_preds.append(preds.cpu().numpy())\n",
    "    \n",
    "    test_preds = np.concatenate(test_preds)\n",
    "    all_preds.append(test_preds)\n",
    "\n",
    "preds = np.mean(all_preds, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = pd.read_parquet(CFG.TEST_PATH)\n",
    "tst['binds'] = 0\n",
    "tst.loc[tst['protein_name']=='BRD4', 'binds'] = preds[(tst['protein_name']=='BRD4').values, 0]\n",
    "tst.loc[tst['protein_name']=='HSA', 'binds'] = preds[(tst['protein_name']=='HSA').values, 1]\n",
    "tst.loc[tst['protein_name']=='sEH', 'binds'] = preds[(tst['protein_name']=='sEH').values, 2]\n",
    "tst[['id', 'binds']].to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
