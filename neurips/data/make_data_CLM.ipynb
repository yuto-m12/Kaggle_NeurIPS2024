{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "import yaml\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import shutil\n",
    "from time import time\n",
    "import typing as tp\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import StratifiedGroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import average_precision_score as APS\n",
    "import duckdb\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.cuda import amp\n",
    "from torch.nn import BCELoss\n",
    "\n",
    "\n",
    "import timm\n",
    "from mamba_ssm import Mamba\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "# use one device only\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "con = duckdb.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    NUM = 20000\n",
    "    LR = 0.001\n",
    "    WD = 1e-4\n",
    "    NBR_FOLDS = 5\n",
    "    SELECTED_FOLDS = [0, 1, 2, 3, 4]\n",
    "    TRAIN_ENC_PATH = Path('../../data/external/train_enc.parquet')\n",
    "    TEST_ENC_PATH = Path('../../data/external/test_enc.parquet')\n",
    "    TRAIN_PATH = Path('../../data/raw/train.parquet')\n",
    "    TEST_PATH = Path('../../data/raw/test.parquet')\n",
    "    OUTPUT_PATH = Path(f'../../data/processed/{NUM}_50per_CLM.parquet')\n",
    "    BATCH_SIZE = 128\n",
    "    EPOCHS = 5\n",
    "    PATIENCE = 5\n",
    "    REDUCE_LR_PATIENCE = 3\n",
    "    REDUCE_LR_FACTOR = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = con.query(f\"\"\"(SELECT *\n",
    "#                         FROM parquet_scan('{CFG.TRAIN_PATH}')\n",
    "                        \n",
    "#                         LIMIT 60000)\"\"\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = con.query(f\"\"\"(SELECT *\n",
    "                        FROM parquet_scan('{CFG.TRAIN_PATH}')\n",
    "                        WHERE binds = 0\n",
    "                        ORDER BY random()\n",
    "                        LIMIT {CFG.NUM/2})\n",
    "                        UNION ALL\n",
    "                        (SELECT *\n",
    "                        FROM parquet_scan('{CFG.TRAIN_PATH}')\n",
    "                        WHERE binds = 1\n",
    "                        ORDER BY random()\n",
    "                        LIMIT {CFG.NUM/2})\"\"\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train.head())\n",
    "display(train.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = train['molecule_smiles']#.unique()\n",
    "print(len(smiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained ChemBERTa model checkpoint and tokenizer\n",
    "cb_tokenizer = AutoTokenizer.from_pretrained('DeepChem/ChemBERTa-10M-MLM')\n",
    "cb_model = AutoModel.from_pretrained('DeepChem/ChemBERTa-10M-MLM')\n",
    "cb_model.eval()\n",
    "\n",
    "# tokenize SMILES\n",
    "cb_encoded_inputs = cb_tokenizer(list(smiles), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# calculate embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = cb_model(**cb_encoded_inputs)\n",
    "\n",
    "# extract pooled output\n",
    "cb_embeddings = outputs.pooler_output\n",
    "\n",
    "cb_embeddings_df = pd.DataFrame(cb_embeddings.numpy())\n",
    "cb_embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_repeated = cb_embeddings_df.loc[cb_embeddings_df.index.repeat(3)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_embeddings_df = pd.concat([train['id'], cb_embeddings_df], axis=1)\n",
    "binds = train[['binds', 'protein_name']]\n",
    "binds['bind1'] = train.apply(lambda row: row['binds'] if row['protein_name'] == 'BRD4' else 0, axis=1)\n",
    "binds['bind2'] = train.apply(lambda row: row['binds'] if row['protein_name'] == 'HSA' else 0, axis=1)\n",
    "binds['bind3'] = train.apply(lambda row: row['binds'] if row['protein_name'] == 'sEH' else 0, axis=1)\n",
    "cb_embeddings_df = pd.concat([cb_embeddings_df, binds], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cb_embeddings_df.head())\n",
    "display(cb_embeddings_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_embeddings_df.to_parquet(CFG.OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
